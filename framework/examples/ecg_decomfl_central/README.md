# DeComFL: Centralized ECG Classification

Implementation of **DeComFL (Decomposed Communication in Federated Learning)** for ECG binary classification using zeroth-order optimization. This implementation follows **Algorithms 3 and 4** from the paper "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization".

## üéØ Overview

DeComFL achieves dimension-free communication in federated learning by:
- Using **zeroth-order optimization** instead of backpropagation
- Communicating only **gradient scalars** instead of full gradients
- Achieving **99.9% communication savings** for large models
- Maintaining **competitive accuracy** (96.60% on ECG dataset)

### Key Results
- **Test Accuracy**: 96.60%
- **Test Loss**: 0.0948
- **Training Speed**: ~0.04s per round
- **Model Size**: 13,314 parameters
- **Communication**: Only 10 scalars per round (vs 13,314 gradients in traditional FL)

## üìÅ Project Structure

```
ecg_decomfl_central/
‚îú‚îÄ‚îÄ config.py              # Hyperparameters and configuration
‚îú‚îÄ‚îÄ model.py               # ECG classification model (MLP)
‚îú‚îÄ‚îÄ data.py                # Data loading and preprocessing
‚îú‚îÄ‚îÄ run_server.py          # Server implementation (Algorithm 3)
‚îú‚îÄ‚îÄ run_client.py          # Client implementation (Algorithm 4)
‚îú‚îÄ‚îÄ README.md              # This file
‚îú‚îÄ‚îÄ ecg_data/
‚îÇ   ‚îî‚îÄ‚îÄ ecg.csv           # ECG dataset (4997 samples, 140 features)
‚îú‚îÄ‚îÄ data_splits/           # Cached data splits
‚îú‚îÄ‚îÄ checkpoints/           # Saved model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ model_round_50.pth
‚îÇ   ‚îú‚îÄ‚îÄ model_round_100.pth
‚îÇ   ‚îî‚îÄ‚îÄ model_final.pth
‚îî‚îÄ‚îÄ results/
    ‚îî‚îÄ‚îÄ training_history.npz
```

## üî¨ Algorithm Overview

### DeComFL vs Traditional Federated Learning

| Aspect | Traditional FL | DeComFL |
|--------|---------------|---------|
| Gradient Computation | Backpropagation | Zeroth-order (function evaluations) |
| Communication | Full gradients (d parameters) | Gradient scalars (P scalars) |
| Bandwidth | O(d) | O(P) where P << d |
| Privacy | Gradient leakage risk | Enhanced privacy (only scalars) |

### Algorithm 3: Server Side

```
Input: Learning rate Œ∑, smoothing Œº, perturbations P, local steps K, rounds R
1. Initialize global model x‚ÇÄ
2. Initialize gradient history G = ‚àÖ, seed history S = ‚àÖ
3. for r = 0 to R-1 do
4.     Generate random seeds {s^k_{r,p}} for k=1..K, p=1..P
5.     Send seeds to clients
6.     Receive gradient scalars {g^k_{i,r,p}} from clients
7.     Store gradients: G ‚Üê G ‚à™ {g^k_{i,r,p}}
8.     Store seeds: S ‚Üê S ‚à™ {s^k_{r,p}}
9.     Update model using aggregated gradients
10. end for
```

### Algorithm 4: Client Side

```
Input: Seeds {s^k_{r,p}}, current model x
1. Store initial model: x_init ‚Üê x
2. for k = 1 to K do
3.     Initialize delta ‚Üê 0
4.     Sample data batch Œæ
5.     for p = 1 to P do
6.         Generate perturbation: z^k_{r,p} ~ N(0, I_d) from seed s^k_{r,p}
7.         Compute gradient scalar: g = (f(x + Œºz; Œæ) - f(x; Œæ)) / Œº
8.         Accumulate: delta ‚Üê delta + g ¬∑ z
9.     end for
10.    Update model: x ‚Üê x - (Œ∑/P) ¬∑ delta
11. end for
12. Revert model: x ‚Üê x_init  (CRITICAL STEP)
13. Return gradient scalars {g^k_{i,r,p}}
```

## üöÄ Quick Start

### Prerequisites

```bash
# Python 3.11+ recommended
# Required packages
torch>=2.0.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
matplotlib>=3.7.0  # Optional, for visualization
```

### Installation

```bash
# Clone or navigate to the project directory
cd ecg_decomfl_central

# Install dependencies
pip install torch numpy pandas scikit-learn matplotlib

# Or using conda
conda install pytorch numpy pandas scikit-learn matplotlib -c pytorch
```

### Data Preparation

Place your ECG data in `ecg_data/ecg.csv`:
- **Format**: CSV file with features and label
- **Features**: Columns 0-139 (140 features)
- **Label**: Column 140 (binary: 0=Normal, 1=Abnormal)

```csv
feature_0,feature_1,...,feature_139,label
0.234,0.567,...,0.891,0
0.123,0.456,...,0.789,1
...
```

### Training

```bash
python run_server.py
```

**Expected Output:**
```
======================================================================
DeComFL CENTRALIZED TRAINING
======================================================================
...
Round 10/100 | Loss: 0.3530 | Acc: 92.00% | Time: 0.04s
Round 20/100 | Loss: 0.2502 | Acc: 93.30% | Time: 0.04s
...
Round 100/100 | Loss: 0.0948 | Acc: 96.60% | Time: 0.04s
======================================================================
TRAINING COMPLETED
======================================================================
Final Results:
  Test Loss: 0.0948
  Test Accuracy: 96.60%
  Best Accuracy: 96.60%
```

## ‚öôÔ∏è Configuration

Edit `config.py` to customize training:

### DeComFL Hyperparameters

```python
# Algorithm Parameters
NUM_ROUNDS = 100          # R: Number of communication rounds
NUM_LOCAL_STEPS = 1       # K: Local SGD steps per round
NUM_PERTURBATIONS = 10    # P: Number of random perturbations
LEARNING_RATE = 0.001     # Œ∑: Learning rate
SMOOTHING_PARAM = 0.001   # Œº: Smoothing parameter for gradient estimation

# Data Configuration
DATA_FRACTION = 1.0       # Use 100% of data (0.1 = 10%)
TEST_SIZE = 0.2           # 20% for testing

# Model Architecture
HIDDEN_DIM = 64           # Hidden layer dimension
NUM_CLASSES = 2           # Binary classification

# Training Configuration
BATCH_SIZE_TRAIN = 128    # Training batch size
BATCH_SIZE_TEST = 256     # Test batch size
DEVICE = 'cuda'           # 'cuda' or 'cpu'
SEED = 42                 # Random seed
```

### Hyperparameter Tuning Guide

| Parameter | Effect | Recommended Range |
|-----------|--------|-------------------|
| `NUM_PERTURBATIONS` (P) | More = better gradient estimates but slower | 5-20 |
| `LEARNING_RATE` (Œ∑) | Higher = faster but less stable | 0.0001-0.01 |
| `SMOOTHING_PARAM` (Œº) | Smaller = more accurate but unstable | 0.0001-0.01 |
| `NUM_LOCAL_STEPS` (K) | More = less communication but potential divergence | 1-10 |

## üìä Understanding the Results

### Output Files

1. **Checkpoints** (`checkpoints/`)
   - `model_round_50.pth`: Checkpoint at round 50
   - `model_round_100.pth`: Checkpoint at round 100
   - `model_final.pth`: Final trained model

2. **Results** (`results/`)
   - `training_history.npz`: Contains arrays for rounds, test_loss, test_accuracy, train_time

### Loading Saved Models

```python
import torch
from model import create_model

# Load checkpoint
checkpoint = torch.load('checkpoints/model_final.pth')

# Create model
model = create_model(
    input_dim=140,
    hidden_dim=64,
    num_classes=2,
    device='cuda'
)

# Load weights
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Use for inference
with torch.no_grad():
    predictions = model(your_ecg_data)
```

### Visualizing Results

```python
import numpy as np
import matplotlib.pyplot as plt

# Load training history
data = np.load('results/training_history.npz')
rounds = data['rounds']
test_loss = data['test_loss']
test_accuracy = data['test_accuracy']

# Plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

ax1.plot(rounds, test_loss)
ax1.set_xlabel('Round')
ax1.set_ylabel('Test Loss')
ax1.set_title('Training Loss')
ax1.grid(True)

ax2.plot(rounds, test_accuracy)
ax2.set_xlabel('Round')
ax2.set_ylabel('Accuracy (%)')
ax2.set_title('Test Accuracy')
ax2.grid(True)

plt.tight_layout()
plt.savefig('training_curves.png', dpi=150)
plt.show()
```

## üîç Technical Details

### Model Architecture

```
ECGModel (13,314 parameters)
‚îú‚îÄ‚îÄ fc1: Linear(140 ‚Üí 64)     # 9,024 params
‚îú‚îÄ‚îÄ relu1: ReLU()
‚îú‚îÄ‚îÄ dropout1: Dropout(0.3)
‚îú‚îÄ‚îÄ fc2: Linear(64 ‚Üí 64)      # 4,160 params
‚îú‚îÄ‚îÄ relu2: ReLU()
‚îú‚îÄ‚îÄ dropout2: Dropout(0.3)
‚îî‚îÄ‚îÄ fc3: Linear(64 ‚Üí 2)       # 130 params
```

### Zeroth-Order Gradient Estimation

For each perturbation p:
1. Sample random direction: `z ~ N(0, I_d)`
2. Evaluate loss at current point: `f(x; Œæ)`
3. Evaluate loss at perturbed point: `f(x + Œºz; Œæ)`
4. Compute gradient scalar: `g = (f(x + Œºz; Œæ) - f(x; Œæ)) / Œº`
5. Update direction: `delta += g ¬∑ z`

Final update: `x ‚Üê x - (Œ∑/P) ¬∑ delta`

### Communication Efficiency

**Traditional FL:**
- Sends full gradient vector: 13,314 float32 values
- Communication cost: 13,314 √ó 4 bytes = 53.3 KB

**DeComFL:**
- Sends P gradient scalars: 10 float32 values
- Communication cost: 10 √ó 4 bytes = 40 bytes
- **Savings: 99.92%** üéâ

### Privacy Advantages

1. **Gradient obfuscation**: Only scalars, not full gradients
2. **No reconstruction**: Cannot recover original data from scalars
3. **Differential privacy**: Easier to add noise to scalars than gradients

## üß™ Experiments & Extensions

### 1. Compare with Standard SGD

Create a baseline with traditional backpropagation:

```python
# train_baseline.py
import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.001)
for epoch in range(100):
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
```

### 2. Vary Number of Perturbations

Test different P values:
```python
# In config.py, try:
NUM_PERTURBATIONS = 5   # Faster, less accurate
NUM_PERTURBATIONS = 10  # Balanced (default)
NUM_PERTURBATIONS = 20  # Slower, more accurate
```

### 3. Multi-Client Federated Learning

To extend to real federated learning:

```python
# In config.py
NUM_CLIENTS = 5  # Split data across 5 clients
ALPHA = 0.5      # Non-IID parameter

# Data will be automatically split using Dirichlet distribution
```

### 4. Different Smoothing Parameters

```python
# Experiment with Œº values
SMOOTHING_PARAM = 0.01    # Larger, more stable
SMOOTHING_PARAM = 0.001   # Default
SMOOTHING_PARAM = 0.0001  # Smaller, more accurate but noisy
```

## üìà Performance Benchmarks

### Training Performance (100 rounds)

| Metric | Value |
|--------|-------|
| Initial Accuracy | 49.60% |
| Final Accuracy | 96.60% |
| Improvement | +47.00% |
| Time per Round | ~0.04s |
| Total Training Time | ~4s |
| Model Size | 13,314 params |
| Dataset Size | 4,997 samples |

### Accuracy Progression

| Round | Loss | Accuracy |
|-------|------|----------|
| 0 | 0.6995 | 49.60% |
| 10 | 0.3530 | 92.00% |
| 20 | 0.2502 | 93.30% |
| 50 | 0.1521 | 94.80% |
| 100 | 0.0948 | 96.60% |

## üêõ Troubleshooting

### Issue: Out of Memory
```
RuntimeError: CUDA out of memory
```
**Solution**: Reduce batch size in `config.py`:
```python
BATCH_SIZE_TRAIN = 64  # or 32
```

### Issue: Slow Training
**Solutions**:
1. Reduce `NUM_PERTURBATIONS` (try P=5)
2. Use GPU if available (set `DEVICE='cuda'`)
3. Increase batch size (if memory allows)

### Issue: Poor Convergence
**Solutions**:
1. Increase `NUM_PERTURBATIONS` (try P=20)
2. Decrease `LEARNING_RATE` (try Œ∑=0.0001)
3. Adjust `SMOOTHING_PARAM` (try Œº=0.01 or 0.0001)

### Issue: NaN Loss
```
Loss becomes NaN during training
```
**Solutions**:
1. Increase `SMOOTHING_PARAM` (try Œº=0.01)
2. Decrease `LEARNING_RATE` (try Œ∑=0.0001)
3. Check for invalid values in data

## üìö References

### Paper
```bibtex
@article{decomfl2024,
  title={Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization},
  author={[Authors]},
  journal={[Journal/Conference]},
  year={2024}
}
```

### Related Work
- **Federated Learning**: McMahan et al., "Communication-Efficient Learning of Deep Networks from Decentralized Data" (2017)
- **Zeroth-Order Optimization**: Nesterov & Spokoiny, "Random Gradient-Free Minimization of Convex Functions" (2017)
- **ECG Classification**: Hannun et al., "Cardiologist-Level Arrhythmia Detection with Convolutional Neural Networks" (2019)

## ü§ù Contributing

This implementation can be extended in several ways:

### Potential Improvements
1. **Add more optimizers**: Implement Adam, RMSprop variants for zeroth-order
2. **Dynamic perturbations**: Adapt P based on convergence
3. **Advanced aggregation**: Weighted averaging based on client data size
4. **Privacy mechanisms**: Add differential privacy to gradient scalars
5. **Model compression**: Quantize gradient scalars for further communication savings

### Code Style
- Follow PEP 8 guidelines
- Add type hints for function arguments
- Include docstrings for all functions
- Comment complex algorithmic steps

## üìÑ License

This implementation is for research and educational purposes. Please cite the original DeComFL paper if you use this code in your research.

## üôè Acknowledgments

- DeComFL algorithm from the original paper
- ECG dataset preprocessing
- PyTorch framework for deep learning
- Federated learning community

## üìß Contact

For questions or issues:
1. Check the Troubleshooting section
2. Review the paper for algorithm details
3. Examine the code comments for implementation specifics

---

**Last Updated**: January 2026  
**Version**: 1.0.0  
**Status**: Production-ready for centralized training ‚úÖ